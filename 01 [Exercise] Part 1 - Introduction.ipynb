{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Blockcourse - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting your feet wet with python and spike train data.\n",
    "\n",
    "#### execute this first:\n",
    "(Note: `import` statements seem to run very slowly on the CIP machines. Be patient, the other cells will run more quickly :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 a)\n",
    "\n",
    "You are given some spike data as a list of spike times in milliseconds. But that is a cumbersome format - we prefer our spike trains in the form of binary vectors of fixed length.\n",
    "\n",
    "Create a 1D array called `spikes` whose number of entries is `trial_length` and which is 1 at every time step where a spike was recorded, 0 everywhere else.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trial_length = 1000\n",
    "spike_times = [50, 133, 141, 217, 227, 297, 370, 409, 418, 421, 423, 428, 470, 487, 541, 555, 556, 562, 637, 639, 732, 745, 758, 816, 825, 828, 829, 842, 862, 873, 874, 932]\n",
    "# spikes = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint 1:** python's `for` loops iterate over lists, like this:\n",
    "\n",
    "    number_list = [1,2,3,4]\n",
    "    for number in number_list:\n",
    "        (do something with number)\n",
    "\n",
    "In each step of the loop, the control variable (here `number`) takes on the next value in the list over which you iterate (here `number_list`).\n",
    "\n",
    "**Hint 2:** Use square brackets for indexing into numpy arrays:\n",
    "\n",
    "    number_list[2] = number_list[4] # sets the 3rd value equal to the 5th\n",
    "\n",
    "Note that indices start at 0.\n",
    "\n",
    "**Hint 3:**\n",
    "You get the shortest solution by using multiple indices at once. See http://docs.scipy.org/doc/numpy/user/basics.indexing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "The plot below shows the spike train (red lines) and its convolution with a gaussian kernel (dotted line). The latter is a smoothed version of the spike train, so it provides an estimation of how densely the spikes are packed next to each other at each point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "figure(figsize=(12,4))\n",
    "plot(np.convolve(spikes,signal.gaussian(100, std=20), mode='same'),':k')\n",
    "plot(spike_times,np.zeros(len(spike_times)),'|r',ms=70)\n",
    "xlabel(\"time step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 b)\n",
    "\n",
    "This excercise is about creating a random spike train. Concretely, you will sample one realisation of a [Bernoulli process](http://en.wikipedia.org/wiki/Bernoulli_process) with fixed firing probability.\n",
    "\n",
    "You are given a firing probability and an array of 1000 random numbers, taken from a uniform distribution between zero and one. We need a binary array called `spikes` with length 1000, which, at each step, is 1 with the given probability and 0 otherwise.\n",
    "\n",
    "After you found out how to do this in principle, here are some relevant Python hints:\n",
    "\n",
    "**Hint 1:** One approach involves python's `enumerate` construct. [Read about it here](https://docs.python.org/2/library/functions.html#enumerate). It exists to iterate over values from a list while *also* keeping track of an increasing index variable.\n",
    "\n",
    "** Hint 2:** If you apply a logical operation to a given numpy array, the result is a boolean (logical) array of the same length. This leads to a one-line solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trial_length = 1000\n",
    "random_numbers = np.random.rand(trial_length)\n",
    "p_firing = 0.03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spikes = ...\n",
    "\n",
    "figure(figsize=(8,2))\n",
    "plot(spikes,'|r',ms=50)\n",
    "title(\"your sampled spike train\")\n",
    "ylim([0.5,1.5])\n",
    "yticks([])\n",
    "xlabel(\"step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 c)\n",
    "Print the total number of spikes and the mean firing rate of that spike train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 d)\n",
    "\n",
    "Above, you took a single sample of a Bernoulli process. Now, let's take repeated trials, i.e. several independent samples.\n",
    "\n",
    "* turn `spikes` into a two-dimensional matrix with as many rows as there are repeated trials, and as many columns as there are time steps. Each row should be one realisation of a Bernoulli process with the given firing probability, i.e. one spike train as above.\n",
    "\n",
    "\n",
    "* Each spike train has an average firing rate. Plot a histogram of these firing rates across trials.\n",
    "\n",
    "* Compute the mean firing rate over all trials and show it superimposed on the histogram.\n",
    "\n",
    "**Hint 1:** as mentioned, `for` loops always iterate over lists. To get a \"traditional\" `for` loop with just a counting variable, you can use the function `range` to create a list of increasing indices.\n",
    "\n",
    "**Hint 2:** `np.mean()` and `np.sum()` accept a parameter to specify over which dimension to accumulate. The default behaviour is to accumulate accross all dimensions, resulting in a single scalar value.\n",
    "\n",
    "**Hint 3:** `random.rand()` allows to create arrays with more than one dimension. You can create the whole matrix of spike trains without using a `for` loop. \n",
    "\n",
    "\n",
    "**Plotting hints:**\n",
    "\n",
    "[Here is a tutorial about plotting](http://nbviewer.ipython.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb) with many example pictures. Here is [the official documentation of the plotting library](http://matplotlib.org/api/pyplot_api.html). But remember that you can access documentation right from within the ipython notebook, too -- for example, you can just type `hist(*shift-TAB* *shift-TAB* *shift-TAB*...` or you can execute the line `hist?`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trial_length = 1000\n",
    "trials = 100\n",
    "p_firing = 0.03\n",
    "\n",
    "\n",
    "\n",
    "assert spikes.shape == (trials,trial_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 e)\n",
    "\n",
    "You probably noticed that most realisations of our random process (i.e. most of the collected spike trains) have a spiking frequency that is different from the value that we defined as the spiking probability.\n",
    "\n",
    "We now try to find out how likely it is that these spike trains were generated from a process with a different firing probability. To do this, we iterate over many different firing probabilities (in some interval), and for each compute the likelihood that the corresponding Bernoulli process generated our spike trains.\n",
    "\n",
    "* create a list called `p_range` that holds possible values for p between `0.001` and `0.1`, with steps of size `0.001` between them.\n",
    "* implement the function `loglike` that returns the log-likelihood of some value of p, given a spike train.\n",
    "\n",
    "**Hint 1:** To compute the log-likelihood of a Bernoulli process with parameter p for a given spiketrain, remember that a spike train consists of many independent variables (one at each time bin). Also, the log-likelihood function of a single [Bernoulli distributed](http://en.wikipedia.org/wiki/Bernoulli_distribution) variable is given by: $$ l(p;x)=x \\cdot \\text{log} (p) + (1−x) \\text{log} (1−p) $$\n",
    "where $x$ is the binary outcome, $p$ the success probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a list of possible values for p:\n",
    "# p_range = np.arange(...\n",
    "\n",
    "def loglike(p,xs):\n",
    "    \"\"\" given a firing probability p and a spike train xs, this function\n",
    "    should return the log-likelihood that the spike train was generated\n",
    "    by a Bernoulli process with the given parameter p.\n",
    "    \n",
    "    Arguments:\n",
    "        xs: 1D binary array of length trial_length\n",
    "        p: scalar\n",
    "    \n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n",
    "# Read this to see how the function you're writing will be used:\n",
    "figure(figsize=(12,5))\n",
    "for t in range(trials):\n",
    "    loglikes = np.zeros(len(p_range))\n",
    "    for i,p in enumerate(p_range):\n",
    "        loglikes[i] = loglike(p,spikes[t,:]) # <--- the likelihood calculation!\n",
    "    plot(p_range,loglikes)\n",
    "    \n",
    "title('Loglikelihood curves for all trials')\n",
    "xlabel('p')\n",
    "ylabel('LL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 f)\n",
    "\n",
    "Let's analyze these curves.\n",
    "\n",
    "* In each trial, search for the value $\\hat{p}$ with the highest likelihood, i.e. the maximum likelihood estimate of p.\n",
    "* Create a histogram of these estimated values of p across all trials. Print their mean.\n",
    "\n",
    "**Hint 1:** Look into `np.argmax`, which gives you the *index* at which an array is maximal.\n",
    "\n",
    "**Hint 2:** There are many ways solve this, but perhaps you find it helpful to create a growing list. To add values to a list, initialise an empty list like so: `mylist = []`. Then, for example in a for loop, you can append a value to the list like this: `mylist.append(value)`. \n",
    "[List comprehensions](https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions) are a more concise notation for this.\n",
    "\n",
    "\n",
    "**Question:** What is the the relationship between the the true firing probability and the mean maximum likelihood estimator of p that you just printed? What would you need to do to make the two more similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fd5e6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Fluctuating Rate\n",
    "-------------\n",
    "\n",
    "Until now, we assumed the same firing probability in each time step. Now, let's look at spike trains where the firing probability changes over time.\n",
    "\n",
    "First, we generate some spike trains whose firing probability varies. To this end, we define a function that provides a spike probability for each time step. Let's say the function consists of two sine waves with different frequencies, summed together with weights $\\alpha$ and $\\beta$.\n",
    "\n",
    "Your task will be to take these spike trains and estimate the combination of parameters $\\hat{\\alpha}$ and $\\hat{\\beta}$ that are most likely to explain the given spike trains, and compare these estimates with the true values of $\\alpha$ and $\\beta$.\n",
    "\n",
    "First, run the next two cells to generate the spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_alpha = 0.05\n",
    "original_beta = 0.03\n",
    "\n",
    "print \"true values:\",original_alpha, original_beta\n",
    "\n",
    "def p_t(alpha,beta):\n",
    "    \"\"\"returns some wavy curve between 0 and 1\"\"\"\n",
    "    wave = alpha * np.sin(np.arange(trial_length)/20.0) + beta * np.sin(np.arange(trial_length)/30.0)\n",
    "    wave[wave>0.999] = 0.999\n",
    "    wave[wave<0.001] = 0.001\n",
    "    return wave\n",
    "\n",
    "plot(p_t(original_alpha,original_beta))  \n",
    "title('True firing probabilities over time')\n",
    "xlabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trial_length = 1000\n",
    "trials = 12\n",
    "\n",
    "spikes = np.zeros((trials,trial_length))\n",
    "for trial in range(trials):\n",
    "    random_numbers = np.random.rand(trial_length)\n",
    "    spikes[trial,random_numbers  < p_t(original_alpha,original_beta)] = 1.0\n",
    "\n",
    "figure()\n",
    "plot(sum(spikes,0))\n",
    "xlabel('Time')\n",
    "title('Sampled Spike Count over time, all trials')\n",
    "print \"true alpha:\", original_alpha, \"true beta:\",original_beta\n",
    "\n",
    "\n",
    "figure(figsize=(10,2))\n",
    "title(\"Example of a single trial\")\n",
    "plot(spikes[0,:],'|r',ms=50)\n",
    "ylim([0.5,1.5])\n",
    "yticks([])\n",
    "xlabel(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 a)\n",
    "\n",
    "How much information about the changing firing probability can we recover from a single spike train? To find out, we want to estimate the parameters $\\alpha$ and $\\beta$ individually for each spike train.\n",
    "\n",
    "To do this, we will again search over a range of possible parameters and compute the likelihoods for each. \n",
    " \n",
    " * Define some plausible intervals over which to search for $\\alpha$ and $\\beta$.\n",
    "  \n",
    "All possible progressions of the firing probability over time are then plotted below.\n",
    "\n",
    "**Hint:** If the cell takes too long to execute, interrupt the kernel and choose a smaller search space (smaller ranges or wider steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use these two arrays to define your search space. \n",
    "# One should contain all values for alpha we want\n",
    "# to try, the other all values for beta.\n",
    "# alpha_range = ...\n",
    "# beta_range = ...\n",
    "\n",
    "\n",
    "assert len(alpha_range) * len(beta_range) <= 10000\n",
    "\n",
    "figure(figsize=(10,5))\n",
    "for b in beta_range:\n",
    "    for a in alpha_range:\n",
    "        plot(p_t(a,b),alpha=1.0/np.sqrt(len(alpha_range) * len(beta_range)))\n",
    "\n",
    "title('all possible spike probability progressions in the given parameter search space.')\n",
    "xlabel('time')\n",
    "ylabel('spike probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these possible evolutions of firing probabilities can explain a given spike train better than others, and now we want to find which ones those are.\n",
    "\n",
    "* Implement the function `ll(ps,xs)` which computes the log-likelihood of a spike train `xs`, under the assumption that the spike train was generated by a Bernoulli process, parameterized by the vector `ps` which provides a different firing probabilities at each time step. \n",
    "\n",
    "**Hint:** This works similarly to the case above where we had the same, fixed firing probability in all time steps. Think of each time bin as an independent, bernoulli distributed random variable with some particular spiking probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ll(ps,xs):\n",
    "    \"\"\"returns the log likelihood of a spike train xs\n",
    "    given a sequence of firing probabilities ps.\n",
    "    \n",
    "    Arguments:\n",
    "        ps: 1D float array of length trial_length\n",
    "        xs: 1D binary array of length trial_length\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "# Read this to see how the function you're writing will be used:\n",
    "figure(figsize=(12,10))\n",
    "plot_trials = range(12)\n",
    "firstplot = True\n",
    "for trial_nr in plot_trials:\n",
    "    likelihoods = np.zeros((len(alpha_range),len(beta_range)))\n",
    "    for ia,a in enumerate(alpha_range):\n",
    "        for ib,b in enumerate(beta_range):\n",
    "            likelihoods[ia,ib] = ll(p_t(a,b),spikes[trial_nr,:]) # <--- the likelihood calculation!\n",
    "    \n",
    "    argmx = np.argmax(likelihoods) # returns an index into the flattened version of the array (a single number)\n",
    "    argmx_indices = np.unravel_index(argmx,likelihoods.shape) # \"unflattens\" the index (so we have an ij tuple again)\n",
    "    max_alpha = alpha_range[argmx_indices[0]]\n",
    "    max_beta = beta_range[argmx_indices[1]]       \n",
    "    # The rest is just plot code:\n",
    "    ax = subplot(3,len(plot_trials)/3,trial_nr+1)\n",
    "    title(r'likelihoods trial '+str(trial_nr)+'\\n '+r'MLE: $\\hat{\\alpha} ='+str(max_alpha)+r'$ $\\hat{\\beta}='+str(max_beta)+r'$')\n",
    "    im = imshow(likelihoods,interpolation='nearest',origin='lower')\n",
    "    im.set_extent([beta_range[0],beta_range[-1],alpha_range[0],alpha_range[-1]])\n",
    "    xticks(rotation=45)\n",
    "    xlabel(r'$\\beta $')\n",
    "    ylabel(r'$\\alpha $')\n",
    "    if not firstplot:\n",
    "        setp(ax.get_xticklabels(), visible=False)\n",
    "        setp(ax.get_yticklabels(), visible=False)\n",
    "    firstplot = False\n",
    "    subplots_adjust(hspace=0.5)\n",
    "print \"true alpha:\", original_alpha, \"true beta:\",original_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 b\n",
    "\n",
    "You can see that you already get some idea about plausible values for $\\alpha$ and $\\beta$ even from looking at a single spike train. Let's refine this estimate by looking at multiple trials: Compute the liklihoods of different $\\alpha$ and $\\beta$ on *all* the spike trains.\n",
    "\n",
    "* implement the function `ll_alltrials(ps,spikes)`. The only difference to the previous one is that it should return the likelihood of the vector of probabilities `ps` given **all** spike trains.\n",
    "\n",
    "* What is the maximum likelihood estimate of $\\alpha$ and $\\beta$?\n",
    "\n",
    "\n",
    "**Hint:** Just like the time bins witin a spike train, the different spike trains are also independent of each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ll_alltrials(ps,spikes):\n",
    "    \"\"\"Returns the likelihood of probability progression ps given\n",
    "    all spike trains.\n",
    "    \n",
    "    Arguments:\n",
    "        ps: 1D float array of length trial_length\n",
    "        spikes: 2D binary array with dimensions nr_trials x trial_length\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "# Read this to see how the function you're writing will be used:\n",
    "likelihoods = np.zeros((len(alpha_range),len(beta_range)))\n",
    "for ia,a in enumerate(alpha_range):\n",
    "    for ib,b in enumerate(beta_range):\n",
    "        likelihoods[ia,ib] = ll_alltrials(p_t(a,b),spikes) # <--- the likelihood calculation!\n",
    "\n",
    "# Plotting:\n",
    "title('likelihoods, all trials')\n",
    "im = imshow(likelihoods,interpolation='nearest',origin='lower')\n",
    "im.set_extent([beta_range[0],beta_range[-1],alpha_range[0],alpha_range[-1]])\n",
    "xticks(rotation=45)\n",
    "xlabel(r'$\\beta $')\n",
    "ylabel(r'$\\alpha $')\n",
    "colorbar()\n",
    "print \"true alpha:\", original_alpha, \"true beta:\",original_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print the maximum likelihood estimate of $\\alpha$ and $\\beta$\n",
    "* Plot the firing probability function `p_t` with those parameter values.\n",
    "\n",
    "**Hint**: The code in the previous exercise (where we computed likelihoods for individual trials) already shows how to extract the maximum values of $\\alpha$ and $\\beta$ from the array `likelihoods`. The task now is just to find that part and adapt it to this situation here :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...\n",
    "print \"MLE of alpha:\",max_alpha,\"\\nMLE of beta:\",max_beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary & Outlook\n",
    "\n",
    "\n",
    "* You created a set of constant-rate spike trains, by sampling several realisations of a Bernoulli process with fixed firing probability.\n",
    "\n",
    "\n",
    "* You calculated the likelihoods of different firing probabilities given these spike trains, then found the most likely one. In other words, you estimated the firing probability of a given set of spike trains, under the assumption that this probability is constant.\n",
    "\n",
    "\n",
    "* We then introduced spike trains with varying firing probability (i.e., Bernoulli processes with varying `p`). You estimated how the firing probability of a given (set of) spiketrains changes over time, under the assumption that this probability evolves according to a particular, known function with two parameters.\n",
    "\n",
    "In the next part, we take these ideas and make them more practical for working with real spike recordings: \n",
    "\n",
    "* Instead of the simple two-parameter function shown here, we will show more flexible models of probability variations over time.\n",
    "* Instead of searching through all parameter combinations, we will introduce a more elegant way of estimating model parameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
